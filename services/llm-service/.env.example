# =============================================================================
# LLM Service Environment Configuration
# =============================================================================

# -----------------------------------------------------------------------------
# Service Configuration
# -----------------------------------------------------------------------------
ENVIRONMENT=development
SERVICE_NAME=llm-service
SERVICE_VERSION=1.0.0

# Server settings
HOST=0.0.0.0
PORT=8000
LOG_LEVEL=DEBUG
WORKERS=1

# CORS (comma-separated origins or * for all)
CORS_ORIGINS=*

# -----------------------------------------------------------------------------
# vLLM Configuration
# -----------------------------------------------------------------------------
# URL of the vLLM server (OpenAI-compatible API)
VLLM_BASE_URL=http://localhost:8001/v1

# Default model to use for inference
# Popular options:
# - TinyLlama/TinyLlama-1.1B-Chat-v1.0 (small, fast)
# - mistralai/Mistral-7B-Instruct-v0.2 (good balance)
# - meta-llama/Llama-2-13b-chat-hf (larger, better quality)
DEFAULT_MODEL=TinyLlama/TinyLlama-1.1B-Chat-v1.0

# Generation defaults
DEFAULT_MAX_TOKENS=512
DEFAULT_TEMPERATURE=0.7
DEFAULT_TOP_P=1.0

# Token limits
MAX_INPUT_TOKENS=4096
MAX_OUTPUT_TOKENS=2048

# Request timeout in seconds
REQUEST_TIMEOUT=120

# -----------------------------------------------------------------------------
# Redis Configuration (for request queuing)
# -----------------------------------------------------------------------------
REDIS_URL=redis://localhost:6379/0
ENABLE_QUEUE=true

# Queue settings
QUEUE_NAME=llm_requests
MAX_QUEUE_SIZE=1000
JOB_TIMEOUT=300

# -----------------------------------------------------------------------------
# Embeddings Configuration
# -----------------------------------------------------------------------------
ENABLE_EMBEDDINGS=true

# Embedding model (sentence-transformers)
# Options:
# - all-MiniLM-L6-v2 (fast, 384 dims)
# - all-mpnet-base-v2 (better quality, 768 dims)
# - paraphrase-multilingual-MiniLM-L12-v2 (multilingual)
DEFAULT_EMBEDDING_MODEL=all-MiniLM-L6-v2

# Batch size for embedding generation
EMBEDDING_BATCH_SIZE=32

# -----------------------------------------------------------------------------
# Authentication (optional)
# -----------------------------------------------------------------------------
# API_KEY=your-api-key-here
# REQUIRE_AUTH=false

# -----------------------------------------------------------------------------
# Monitoring (optional)
# -----------------------------------------------------------------------------
ENABLE_METRICS=false
# METRICS_PORT=9090

# -----------------------------------------------------------------------------
# HuggingFace Configuration (for model downloads)
# -----------------------------------------------------------------------------
# HF_TOKEN=your-huggingface-token
# HF_HOME=/path/to/cache
